"""
Enhanced yt-dlp Helper - N√¢ng c·∫•p v·ªõi retry, cache, v√† parallel processing
Version: 2.0.7+ (Anti-Rate-Limit Edition)
"""

import yt_dlp
import os
import json
import time
import hashlib
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from threading import Lock, Semaphore

class EnhancedYTDLP:
    """
    yt-dlp wrapper v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao:
    - Retry mechanism v·ªõi exponential backoff
    - Cache (l∆∞u k·∫øt qu·∫£)
    - Parallel processing v·ªõi rate limiting
    - User-agent rotation
    - Adaptive delays ƒë·ªÉ tr√°nh b·ªã block
    - Better error handling
    """
    
    # User-agent pool ƒë·ªÉ rotate
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    ]
    
    def __init__(self, cookies_file="facebook_cookies.txt", cache_dir=".cache/ytdlp", 
                 max_workers=5, request_delay=0.5, timeout=30):
        """
        Args:
            cookies_file: Path to cookies file
            cache_dir: Directory to store cache
            max_workers: Maximum parallel workers (default: 5)
            request_delay: Delay between requests in seconds (default: 0.5)
            timeout: Socket timeout in seconds (default: 30)
        """
        self.cookies_file = cookies_file
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Rate limiting settings
        self.max_workers = max_workers
        self.request_delay = request_delay
        self.timeout = timeout
        self.last_request_time = 0
        self.request_lock = Lock()
        
        # Semaphore ƒë·ªÉ gi·ªõi h·∫°n concurrent requests
        self.semaphore = Semaphore(max_workers)
        
        # Base options for yt-dlp
        self.base_opts = {
            'quiet': True,
            'no_warnings': True,
            'simulate': True,
            'extract_flat': False,
            'noplaylist': True,
            'ignoreerrors': True,
            'socket_timeout': timeout,
            'http_headers': {
                'User-Agent': random.choice(self.USER_AGENTS),
            }
        }
        
        # Add cookies if available
        if os.path.exists(cookies_file):
            self.base_opts['cookiefile'] = cookies_file
            print(f"[YTDLP+] ‚úÖ Loaded cookies from {cookies_file}")
        else:
            print(f"[YTDLP+] ‚ö†Ô∏è No cookies file found, running in guest mode")
        
        print(f"[YTDLP+] ‚öôÔ∏è Config: workers={max_workers}, delay={request_delay}s, timeout={timeout}s")
    
    def _get_cache_key(self, url):
        """Generate cache key from URL"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _get_cache_path(self, url):
        """Get cache file path for URL"""
        cache_key = self._get_cache_key(url)
        return self.cache_dir / f"{cache_key}.json"
    
    def _load_from_cache(self, url, max_age_hours=24):
        """
        Load result from cache if available and not expired
        
        Args:
            url: Video URL
            max_age_hours: Maximum cache age in hours
            
        Returns:
            dict or None: Cached result or None if not found/expired
        """
        cache_path = self._get_cache_path(url)
        
        if not cache_path.exists():
            return None
        
        try:
            # Check cache age
            cache_age = time.time() - cache_path.stat().st_mtime
            if cache_age > max_age_hours * 3600:
                print(f"[YTDLP+] ‚è∞ Cache expired ({cache_age/3600:.1f}h old)")
                return None
            
            # Load cache
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"[YTDLP+] ‚úÖ Cache hit: {data.get('title', '')[:50]}...")
            return data
            
        except Exception as e:
            print(f"[YTDLP+] ‚ö†Ô∏è Cache load error: {e}")
            return None
    
    def _save_to_cache(self, url, data):
        """Save result to cache"""
        try:
            cache_path = self._get_cache_path(url)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"[YTDLP+] üíæ Cached result for future use")
        except Exception as e:
            print(f"[YTDLP+] ‚ö†Ô∏è Cache save error: {e}")
    
    
    def _wait_for_rate_limit(self):
        """Enforce rate limiting between requests"""
        with self.request_lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            
            if time_since_last < self.request_delay:
                sleep_time = self.request_delay - time_since_last
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()
    
    def _get_random_user_agent(self):
        """Get a random user agent"""
        return random.choice(self.USER_AGENTS)
    
    def get_video_info(self, url, use_cache=True, max_retries=3):
        """
        Get video info with retry, exponential backoff, and rate limiting
        
        Args:
            url: Video URL
            use_cache: Whether to use cache
            max_retries: Number of retries on failure (default: 3)
            
        Returns:
            dict: {'title': str, 'thumbnail': str, 'success': bool, 'error': str}
        """
        result = {
            'title': None,
            'thumbnail': None,
            'success': False,
            'error': None,
            'url': url
        }
        
        # Try cache first
        if use_cache:
            cached = self._load_from_cache(url)
            if cached:
                return cached
        
        # Acquire semaphore to limit concurrent requests
        with self.semaphore:
            # Try extraction with exponential backoff
            for attempt in range(max_retries + 1):
                try:
                    # Wait for rate limiting
                    self._wait_for_rate_limit()
                    
                    if attempt > 0:
                        # Exponential backoff: 2^attempt seconds (2s, 4s, 8s...)
                        backoff_time = min(2 ** attempt, 30)  # Max 30s
                        print(f"[YTDLP+] üîÑ Retry {attempt}/{max_retries} (waiting {backoff_time}s)...")
                        time.sleep(backoff_time)
                    
                    # Rotate user agent for each retry
                    opts = self.base_opts.copy()
                    opts['http_headers'] = {'User-Agent': self._get_random_user_agent()}
                    
                    with yt_dlp.YoutubeDL(opts) as ydl:
                        info = ydl.extract_info(url, download=False)
                        
                        if info:
                            result['title'] = info.get('title', '').strip()
                            result['thumbnail'] = info.get('thumbnail')
                            result['success'] = True
                            
                            # Save to cache
                            if use_cache and result['title']:
                                self._save_to_cache(url, result)
                            
                            print(f"[YTDLP+] ‚úÖ Extracted: {result['title'][:50]}...")
                            return result
                            
                except Exception as e:
                    error_msg = str(e).lower()
                    result['error'] = str(e)
                    
                    # Check if it's a rate limit error
                    is_rate_limit = any(keyword in error_msg for keyword in 
                                       ['429', 'too many requests', 'rate limit', 'throttle'])
                    
                    if is_rate_limit:
                        print(f"[YTDLP+] ‚ö†Ô∏è Rate limit detected! Slowing down...")
                        # Increase delay temporarily
                        self.request_delay = min(self.request_delay * 1.5, 5.0)
                    
                    if attempt == max_retries:
                        print(f"[YTDLP+] ‚ùå Failed after {max_retries} retries: {e}")
                    else:
                        print(f"[YTDLP+] ‚ö†Ô∏è Attempt {attempt + 1} failed: {e}")
        
        return result
    
    
    def batch_get_videos(self, urls, max_workers=None, use_cache=True, progress_callback=None):
        """
        Get info for multiple videos in parallel with rate limiting
        
        Args:
            urls: List of video URLs
            max_workers: Number of parallel workers (None = use instance setting)
            use_cache: Whether to use cache
            progress_callback: Callback function(current, total, result)
            
        Returns:
            list: List of result dicts
        """
        results = []
        total = len(urls)
        workers = max_workers or self.max_workers
        
        print(f"[YTDLP+] üöÄ Processing {total} URLs with {workers} workers...")
        print(f"[YTDLP+] üìä Rate limit: {self.request_delay}s delay, {self.timeout}s timeout")
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all tasks
            future_to_url = {
                executor.submit(self.get_video_info, url, use_cache): url
                for url in urls
            }
            
            # Collect results as they complete
            for i, future in enumerate(as_completed(future_to_url), 1):
                url = future_to_url[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    # Calculate ETA
                    elapsed = time.time() - start_time
                    avg_time = elapsed / i
                    eta = avg_time * (total - i)
                    
                    # Progress callback
                    if progress_callback:
                        progress_callback(i, total, result)
                    else:
                        status = "‚úÖ" if result['success'] else "‚ùå"
                        print(f"[YTDLP+] [{i}/{total}] {status} {url[:50]}... (ETA: {eta:.1f}s)")
                        
                except Exception as e:
                    print(f"[YTDLP+] ‚ùå Error processing {url}: {e}")
                    results.append({
                        'url': url,
                        'title': None,
                        'success': False,
                        'error': str(e)
                    })
        
        success_count = sum(1 for r in results if r['success'])
        total_time = time.time() - start_time
        avg_time = total_time / total if total > 0 else 0
        
        print(f"[YTDLP+] üèÅ Complete: {success_count}/{total} successful")
        print(f"[YTDLP+] ‚è±Ô∏è Total time: {total_time:.1f}s (avg: {avg_time:.2f}s/video)")
        
        return results
    
    def clear_cache(self, older_than_hours=None):
        """
        Clear cache files
        
        Args:
            older_than_hours: Only clear files older than this (None = all)
        """
        count = 0
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                if older_than_hours is None:
                    cache_file.unlink()
                    count += 1
                else:
                    age = time.time() - cache_file.stat().st_mtime
                    if age > older_than_hours * 3600:
                        cache_file.unlink()
                        count += 1
            except Exception as e:
                print(f"[YTDLP+] ‚ö†Ô∏è Error deleting {cache_file}: {e}")
        
        print(f"[YTDLP+] üóëÔ∏è Cleared {count} cache files")


# ============= DEMO USAGE =============
if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ ENHANCED YT-DLP - DEMO")
    print("=" * 60)
    print()
    
    # Initialize
    ytdlp = EnhancedYTDLP(cookies_file="facebook_cookies.txt")
    
    # Test URLs
    test_urls = [
        "https://www.facebook.com/watch/?v=123456789",
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
    ]
    
    # Test 1: Single video with cache
    print("1Ô∏è‚É£ Testing single video (with cache):")
    result = ytdlp.get_video_info(test_urls[0])
    print(f"   Title: {result['title']}")
    print(f"   Success: {result['success']}")
    print()
    
    # Test 2: Same video again (should hit cache)
    print("2Ô∏è‚É£ Testing cache hit:")
    result = ytdlp.get_video_info(test_urls[0])
    print(f"   Title: {result['title']}")
    print()
    
    # Test 3: Batch processing
    print("3Ô∏è‚É£ Testing batch processing:")
    results = ytdlp.batch_get_videos(test_urls, max_workers=2)
    for i, r in enumerate(results, 1):
        status = "‚úÖ" if r['success'] else "‚ùå"
        print(f"   [{i}] {status} {r['title'][:50] if r['title'] else 'Failed'}...")
    
    print()
    print("=" * 60)
    print("‚úÖ DEMO COMPLETE")
    print("=" * 60)
